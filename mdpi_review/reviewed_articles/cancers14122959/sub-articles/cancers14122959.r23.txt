Reviewer 3 Report
I thank the authors for their extensive work and comprehensive review of the manuscript. The revised manuscript is trying to be responsive to previous review comments. However, the new information added, instead to shed light, it brings more confusion.
Overall the author’s main conclusion in the abstract is overstated and is not supported by many questionable data and experimental design.
As I see the results in the actual setting, adding TTFields treatment demonstrated a toxic effect when added to sorafenib.
While I totally understand that the main reason for sacrificing the rats one day after last TTF, as authors mentioned in the letter “it was non-ethical to continue the study further”, this should have applied only for the rats belonging to control group. Majority of rodent study in cancer research lose the control group early but the treated groups are followed up for much longer to delineate indeed the efficacy vs. toxicity. It is practically impossible to appreciate a therapeutic effect in such a short time and upon monitoring effect for 1 day. Kaplan-Meyer survival curves are mandatory when toxic treatments are combined and compared. Is this indeed an anti-tumor effect or a very toxic effect to the liver (including tumor). Were there any investigation performed in animals to demonstrate a local/systemic toxicity? Were normal liver areas investigated? Were the IHC slides blinded to the pathologist(s)?
The in vivo model appears to be performed only time which question the validity of data. I am not sure why the randomization of rats was unequally, practically 50% in TTF groups vs. control and sorafenib alone. The rigor science recommends at least 2 independent experiments with at least 7 animals randomized per group.
The new western blot data are puzzling me. First of all I do not understand why it is so much variability between GADPH signals in all blots in the paper, especially within the same experiment. The reference protein should have the same intensity since this is the control for equal protein loading. Moreover, many bands belonging to all investigated proteins are truncated, fractured and I identified a lot of troubleshooting in bands due the presence of bubbling when running the blots. When come to densitometry, and especially when we look at the small range of fold changes between groups and controls, these troubleshooting can make huge differences. As a rule of thumb, when a housekeeping protein gives problems in immunoblotting, there are many other classic protein to switch the investigation (b-actin, vinculin, Ku, etc.). Otherwise, all protein investigation appears as a consequence of random effect with some fishing expedition.
Another weird observation is the new figures 3D, E, and F. Despite showing the same fold changes graphs in letter to reviewer and updated manuscript, the bands are different and all over the place in the mentioned files. The bands for 48 hours figure 3D (right panel), 6 and 24 hours for figure 3E and 48 hours for figure F are completely different between the letter and mansucript. What is even more confusing is that in figure 4 for time points 6 and 24 hours, where, despite keeping the same GAPDH bands, the cleaved PARP data are changed. There is no way to analyze bands for a protein using the GAPDH data from another experiment. Which data should be believable? This way to present the data questions a lot the quality of experimental work and consequently and importantly the final analysis of data and the concluion claimed by the authors.
Were these western blots (Figure 3D-F) repeated 3 times as the material and method mentions?
As a final observation, the Conclusion section, the hallmark of any work, is weird and poorly written. It looks like a speculative discussion section with no clear point of view of the authors and general conclusions of their presented work.
